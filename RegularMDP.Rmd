---
title: "Untitled"
author: "Monkie"
date: "11/26/2019"
output: pdf_document
---
read Data
```{r setup, include=FALSE}
rm(list = ls())
# setwd("~/Desktop/AML/cartpole")
setwd("~/Desktop/GITHUB/cartpole/cartpole")
df = read.csv("cartpole.csv")
source("csvfunction.R")
df = CreateSym(df)
```


```{r}
Splitstate = 20
summary(df)
# MAKE QUANTILE
CartPosQuantile  = quantile(df$CartPos,(0:(Splitstate-1))*(1/Splitstate))
CartVelQuantile  = quantile(df$CartVelocity,(0:(Splitstate-1))*(1/Splitstate))
PoleAngleQuantile  = quantile(df$PoleAngle,(0:(Splitstate-1))*(1/Splitstate))
PoleVelocityQuantile  = quantile(df$PoleVelocity,(0:(Splitstate-1))*(1/Splitstate))
# create variable refer to length
car_pos_len = length(CartPosQuantile)
car_vel_len = length(CartVelQuantile)
pol_ang_len = length(PoleAngleQuantile)
pol_vel_len = length(PoleVelocityQuantile)
# MAKE NEW DATAFRAME
df2 = df
# # create penalty
df2$Reward[which(df2$Reward==0)] = 0 # penalty -50
n = length(df2$Step)
df2$CartPos = 0
df2$CartVelocity = 0
df2$PoleAngle = 0
df2$PoleVelocity = 0

# aggregating state 
for (i in 1:car_pos_len){
  df2$CartPos = df2$CartPos + (df$CartPos >= CartPosQuantile[i])
  df2$CartVelocity = df2$CartVelocity + (df$CartVelocity >= CartVelQuantile[i])
  df2$PoleAngle = df2$PoleAngle + (df$PoleAngle >= PoleAngleQuantile[i])
  df2$PoleVelocity = df2$PoleVelocity + (df$PoleVelocity >= PoleVelocityQuantile[i])
}
# Createsummary for each variable
summary(as.factor(df2$CartPos))
summary(as.factor(df2$CartVelocity))
summary(as.factor(df2$PoleAngle))
summary(as.factor(df2$PoleVelocity))
summary(df2)

```

Create average reward ARRAY by LOOPING THROUGH THE DATA AND TAKE AVERAGE FOR EACH ACTION STATE PAIR
```{r}
StateSpace = rep(0,car_pos_len^4)
ExpectedRew1 = StateSpace
Occurence1 = StateSpace
ExpectedRew2 = StateSpace
Occurence2 = StateSpace
# distinguish state
df2$State = ((df2$CartPos-1) + car_pos_len*(df2$CartVelocity-1) + 
  car_pos_len*car_vel_len*(df2$PoleAngle-1) +car_pos_len*car_vel_len*pol_ang_len*(df2$PoleVelocity-1))+1
# create rewawrd by taking average reward
for (i in 1:length(df2$Step)){
  if (df2$Action[i] == 1){
    ExpectedRew1[df2$State[i]] = ExpectedRew1[df2$State[i]] + df2$Reward[i]
    Occurence1[df2$State[i]] = Occurence1[df2$State[i]] + 1    
  }
  if (df2$Action[i] == 0){
    ExpectedRew2[df2$State[i]] = ExpectedRew2[df2$State[i]] + df2$Reward[i]
    Occurence2[df2$State[i]] = Occurence2[df2$State[i]] + 1    
  }
}
data1 = which(Occurence1 >0)
ExpectedRew1[data1] = ExpectedRew1[data1]#/Occurence1[data1] 
data2 = which(Occurence2 >0)
ExpectedRew2[data2] = ExpectedRew2[data2]#/Occurence2[data2] 
# create summry 
summary(ExpectedRew1 )
summary(Occurence1 )
summary(ExpectedRew2 )
summary(Occurence2 )

tot_occurence = Occurence1+Occurence2

stay = which(tot_occurence != 0)
for ( i in  1:length(stay)){
  StateSpace[stay[i]] = i 
}
remove = which(tot_occurence == 0)

# update indexing 
for (i in 1:length(df2$Step) ){
  df2$State[i] = StateSpace[df2$State[i]]
}
```
Create Transition probability
```{r}
# remove state that never been visited.
ExpectedRew1 = ExpectedRew1[-remove]
ExpectedRew1 = c(ExpectedRew1,-500) # reward for terminations state
ExpectedRew2 = ExpectedRew2[-remove]
ExpectedRew2 = c(ExpectedRew2,-500) # reward for terminations state
Occurence1 = Occurence1[-remove]
Occurence1 = c(Occurence1,sum(df2$Action[(which(df2$Step==0)-1)])) # occurence for terminations state
Occurence2 = Occurence2[-remove]
Occurence2 = c(Occurence2,sum(df2$Action[(which(df2$Step==0)-1)]==0)) # occurence for terminations state
# Termination state
Term = c((which(df2$Step[-1]==0)),nobs(df2$State))
sum(df2$Action[Term])
sum(df2$Action[Term]==0)
# Create Transition Matrix
len_state = length(Occurence1) # last state is terminations state
T1 = matrix(0,len_state,len_state)
T2 = matrix(0,len_state,len_state)
# Make a future state matrix
df2$futureState = c(df2$State[-n],len_state)
df2$futureState[which(df2$Step[-1]==0)] = len_state # Termination State
# MAKE TRANSITION PROBABILITY GIVEN each action 
act1 = which(df2$Action==1) 
for (i in act1){
  T1[df2$State[i],df2$futureState[i]]=T1[df2$State[i],df2$futureState[i]]+1
}

act2 = which(df2$Action==0) 
for (i in act2){
  T2[df2$State[i],df2$futureState[i]]=T2[df2$State[i],df2$futureState[i]]+1
}
# making it a pdf
for(i in 1:len_state){
  s1 =sum(T1[i,])
  s2 =sum(T2[i,])
  if ( s1>0 ){
    T1[i,] = T1[i,]/s1
  }
  if ( s2>0 ){
    T2[i,] = T2[i,]/s2
  }
}


summary(rowSums(T1)==1)
summary(rowSums(T2)==1)

discount = 0.95
v = rep(0,len_state)

for (n in 1:200){
  # v2 =pmax(ExpectedRew1 + discount*T1 %*% v, ExpectedRew2 + discount*T2 %*% v)
  # cat(n,max(v2-v),"\n")
  v  =pmax(T1 %*%(ExpectedRew1 + discount* v), T2 %*%(ExpectedRew2 +  discount*  v))
}

result= (T1 %*%(ExpectedRew1 + v))>(T2 %*%(ExpectedRew2 +  v))
# OUTPUT

summary(v)
summary(result)

dirname = paste0("Mdp",Splitstate,"state")
if (!dir.exists(dirname)){
  dir.create(dirname)
}
k=paste0(dirname,"/all")
if (TRUE){
  write.csv(CartPosQuantile,paste0(k,"_CartPositionQuantile.csv"))
  write.csv(CartVelQuantile,paste0(k,"_CartVelQuantile.csv"))
  write.csv(PoleAngleQuantile,paste0(k,"_PoleAngleQuantile.csv"))
  write.csv(PoleVelocityQuantile,paste0(k,"_PoleVelocityQuantile.csv"))
  write.csv(StateSpace,paste0(k,"_StateMap.csv"))
  write.csv(stay,paste0(k,"_StateArr.csv"))
  write.csv(ExpectedRew1,paste0(k,"_Reward1.csv"))
  write.csv(ExpectedRew2,paste0(k,"_Reward2.csv"))
  write.csv(as.numeric(result),paste0(k,"_policy.csv"))
}


# a2 is no penalty :: a is with penalty

```































